{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9912f46-baa9-47d9-98cb-3de8a142a06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import math\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f9db978-21b1-45a0-b66f-cd0b0e9bf0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 123 \n",
    "random.seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e31f78b-57c1-49d6-b4c6-05299dba05f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dis(column, area):\n",
    "    time = 24 * 3600\n",
    "    area_m = area*1000000\n",
    "    new_column = column * (time / area_m)\n",
    "    return new_column\n",
    "\n",
    "\n",
    "def normalize_columns_with_dis(df, area):\n",
    "    for column in df.columns:\n",
    "        if 'dis' in column:\n",
    "            df[column] = normalize_dis(df[column], area)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fadfa242-5cfa-46db-a974-690614d04289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsample_table(stations, filenames, area):\n",
    "    grdc_nos = [str(grdc_no) for grdc_no in stations['grdc_no']]\n",
    "    sub_filenames = [filename for filename in filenames if any(grdc_no in filename for grdc_no in grdc_nos)]\n",
    "    sub_datas = []\n",
    "    for filename in sub_filenames:\n",
    "        sub_data = pd.read_csv(filename)\n",
    "        sub_datas.append(sub_data)\n",
    "    \n",
    "    sub_table = pd.concat(sub_datas, ignore_index=True)\n",
    "    \n",
    "    return sub_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d530586a-3272-4279-bbdb-f5c1ace27fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "file_path_preds = '../R/data/allpredictors/'\n",
    "file_list_preds = os.listdir(file_path_preds)\n",
    "file_paths = [os.path.join(file_path_preds, file) for file in file_list_preds]\n",
    "\n",
    "station_info = pd.read_csv(\"../R/data/stations_rhine_elbe.csv\")\n",
    "\n",
    "rhine_stations = station_info.loc[(station_info[\"sub_reg\"] == 6351) | (station_info[\"sub_reg\"] == 6361)]\n",
    "elbe_stations = station_info.loc[(station_info[\"sub_reg\"] == 6401)]\n",
    "maas_stations = station_info.loc[(station_info[\"sub_reg\"] == 6211)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f58f56-9e76-43d7-a83b-e0a9cf3993d6",
   "metadata": {},
   "source": [
    "## Sampling cross validation Rhine datasetrhine_stations_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64fcc211-b59f-4f33-b7da-b14848fc8e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No overlap\n",
      "No overlap\n",
      "No overlap\n",
      "No overlap\n",
      "No overlap\n"
     ]
    }
   ],
   "source": [
    "# Create an empty dictionary to store the samples\n",
    "train_samples = {}\n",
    "validation_samples = {}\n",
    "\n",
    "# Get the station ids\n",
    "list_station_ids = rhine_stations[\"grdc_no\"].to_list()\n",
    "list_validation = list_station_ids.copy()\n",
    "\n",
    "# Define the number of stations per sample\n",
    "stations_per_sample = 6\n",
    "\n",
    "# Loop five times to create five samples\n",
    "for sample_num in range(1, 6):\n",
    "    # Randomly select 6 unique stations for the current sample\n",
    "    validation_stations = random.sample(list_validation, stations_per_sample)\n",
    "        \n",
    "    # Add the current sample to the test_samples dictionary\n",
    "    validation_samples[sample_num] = validation_stations\n",
    "    \n",
    "    # Remove the test stations from the list of station ids\n",
    "    list_validation = list(set(list_validation) - set(validation_stations))  # Remove the selected values\n",
    "\n",
    "    # Remove the selected stations from the list of station ids\n",
    "    train_stations_id = [station_id for station_id in list_station_ids if station_id not in validation_stations]\n",
    "\n",
    "    # Add the current sample to the train_samples dictionary\n",
    "    train_samples[sample_num] = train_stations_id\n",
    "\n",
    "\n",
    "for sample_num in range(1, 6):\n",
    "    train_stations = train_samples[sample_num]\n",
    "    validation_stations = validation_samples[sample_num]\n",
    "    \n",
    "    # Check if there is any overlap between train and test samples\n",
    "    if any(station_id in validation_stations for station_id in train_stations):\n",
    "        print(f\"Overlap found in Sample {sample_num}\")\n",
    "        print(f\"Train Sample {sample_num}: {train_stations}\")\n",
    "        print(f\"Test Sample {sample_num}: {validation_stations}\")\n",
    "    else:\n",
    "        print(\"No overlap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e9237e-56b9-45ab-a68e-a8e907035437",
   "metadata": {},
   "source": [
    "## Subsample Elbe and Maas as testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff9d8054-6435-41c1-aaba-f97aa9f1ea84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6340300, 6340160]\n",
      "[6340170, 6340130]\n",
      "[6340301, 6340110]\n",
      "[6140400, 6340140]\n",
      "[6340190, 6340180, 6140401]\n"
     ]
    }
   ],
   "source": [
    "list_test = elbe_stations[\"grdc_no\"].to_list()\n",
    "\n",
    "test_samples_elbe = {}\n",
    "\n",
    "# Generate samples with 2 stations each\n",
    "for i in range(5):\n",
    "    sample = random.sample(list_test, 2)\n",
    "    test_samples_elbe[i + 1] = sample\n",
    "    list_test = list(set(list_test) - set(sample))  # Remove the selected values\n",
    "\n",
    "# Generate the last sample with 3 stations\n",
    "sample = random.sample(list_test, 3)\n",
    "test_samples_elbe[5] = sample\n",
    "\n",
    "# Print the generated samples dictionary\n",
    "for sample_num in range(1, 6):\n",
    "    print(test_samples_elbe[sample_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cc42937-9807-42bb-862d-740935f22027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Sample 1: [6421100]\n",
      "Test Sample 2: [6221100]\n",
      "Test Sample 3: [6421101]\n",
      "Test Sample 4: [6221101, 6221102]\n",
      "Test Sample 5: [6421102, 6421500]\n"
     ]
    }
   ],
   "source": [
    "list_test_maas = maas_stations[\"grdc_no\"].to_list()\n",
    "list_test = list_test_maas.copy()\n",
    "\n",
    "test_samples_maas = {}\n",
    "\n",
    "# Generate samples with 1 station each for the first three subsamples\n",
    "for i in range(1, 4):\n",
    "    sample = random.sample(list_test, 1)\n",
    "    test_samples_maas[i] = sample\n",
    "    list_test = list(set(list_test) - set(sample))  # Remove the selected value\n",
    "\n",
    "# Generate the last two samples with 2 stations each\n",
    "sample_4 = random.sample(list_test, 2)\n",
    "test_samples_maas[4] = sample_4\n",
    "list_test = list(set(list_test) - set(sample_4))\n",
    "\n",
    "sample_5 = random.sample(list_test, 2)\n",
    "test_samples_maas[5] = sample_5\n",
    "\n",
    "# Print the test samples\n",
    "for sample_num, selected_stations in test_samples_maas.items():\n",
    "    print(f\"Test Sample {sample_num}: {selected_stations}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c65fdb4-abfa-411b-b6f1-437d6405d96c",
   "metadata": {},
   "source": [
    "# Elbe & Maas subsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fa25ecc-e8ad-4d00-af73-8cce25cefc84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling Subsample 1...\n",
      "0.9579213128550389\n",
      "Finished Subsample 1...\n",
      "Sampling Subsample 2...\n",
      "0.9787188306104901\n",
      "Finished Subsample 2...\n",
      "Sampling Subsample 3...\n",
      "0.9779830308237568\n",
      "Finished Subsample 3...\n",
      "Sampling Subsample 4...\n",
      "0.9521374514215586\n",
      "Finished Subsample 4...\n",
      "Sampling Subsample 5...\n",
      "0.9455346415698839\n",
      "Finished Subsample 5...\n"
     ]
    }
   ],
   "source": [
    "# Iterate over sub-samples\n",
    "# Path configurations\n",
    "data_setup = {1: \"rhine_only_lag6\",\n",
    "              2: \"rhine_elbe_lag6\",\n",
    "              3: \"rhine_maas_lag6\"\n",
    "                }\n",
    "\n",
    "#Select the sampling setup\n",
    "setup_number = 3\n",
    "setup = data_setup[setup_number]\n",
    "\n",
    "output_base_dir = f'../R/data/{setup}/'\n",
    "os.makedirs(output_base_dir, exist_ok=True)\n",
    "\n",
    "for subsample in range(1, 6):\n",
    "    output_dir = os.path.join(output_base_dir, f'subsample_{subsample}')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    print(f'Sampling Subsample {subsample}...')\n",
    "    \n",
    "    ## Subset train stations randomly:\n",
    "    train_stations = train_samples[subsample]\n",
    "    train_stations = station_info[station_info['grdc_no'].isin(train_stations)]\n",
    "   \n",
    "    #train_stations = station_info[station_info['grdc_no'].isin(rhine_stations[\"grdc_no\"].to_list())]\n",
    "\n",
    "    if setup_number == 1:\n",
    "        #Subset test stations for rhine only setup\n",
    "        test_stations = validation_samples[subsample]\n",
    "        test_stations = station_info[station_info['grdc_no'].isin(test_stations)]\n",
    "    elif setup_number == 2:\n",
    "        \n",
    "        # Subset test stations elbe\n",
    "        test_stations = test_samples_elbe[subsample]\n",
    "        test_stations = station_info[station_info['grdc_no'].isin(test_stations)]\n",
    "        \n",
    "    elif setup_number == 3:\n",
    "        \n",
    "        # Subset test stations \n",
    "        test_stations = test_samples_maas[subsample]\n",
    "        test_stations = station_info[station_info['grdc_no'].isin(test_stations)]\n",
    "\n",
    "        \n",
    "     # Create train table\n",
    "    train_table = subsample_table(train_stations, file_paths, station_info)\n",
    "    train_table['datetime'] = pd.to_datetime(train_table['datetime']).dt.date\n",
    "    \n",
    "    # Create train table\n",
    "    test_table = subsample_table(test_stations, file_paths,station_info)\n",
    "    test_table['datetime'] = pd.to_datetime(test_table['datetime']).dt.date\n",
    "    \n",
    "\n",
    "\n",
    "    nrow_train = train_table.shape[0]\n",
    "    nrow_test = test_table.shape[0]\n",
    "    \n",
    "    ratio_subsamples = nrow_train / (nrow_train + nrow_test)\n",
    "    \n",
    "    print(ratio_subsamples)\n",
    "    \n",
    "    # Sample file paths for test stations\n",
    "    test_file_paths = random.sample(file_paths, k=len(test_stations))\n",
    "    \n",
    "    # Filter file paths for train stations\n",
    "    train_file_paths = [file_path for file_path in file_paths if file_path not in test_file_paths]\n",
    "    \n",
    "    # Write tables: train_stations, test_stations, validation_stations, train_table, test_table, validation_table\n",
    "    train_stations.to_csv(os.path.join(output_dir, 'train_stations.csv'), index=False)\n",
    "    test_stations.to_csv(os.path.join(output_dir, 'test_stations.csv'), index=False)\n",
    "\n",
    "    train_table.to_csv(os.path.join(output_dir, 'train_table_allpredictors.csv'), index=False)\n",
    "    test_table.to_csv(os.path.join(output_dir, 'test_table_allpredictors.csv'), index=False)\n",
    "    \n",
    "    # Save test file paths\n",
    "    with open(os.path.join(output_dir, 'test_file_paths.txt'), 'w') as f:\n",
    "        for file_path in test_file_paths:\n",
    "            f.write(file_path + '\\n')\n",
    "    \n",
    "     \n",
    "    # Save train file paths\n",
    "    with open(os.path.join(output_dir, 'train_file_paths.txt'), 'w') as f:\n",
    "        for file_path in train_file_paths:\n",
    "            f.write(file_path + '\\n')\n",
    "\n",
    "    print(f'Finished Subsample {subsample}...')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c85a938-e92b-4be7-8182-6bedb28a1293",
   "metadata": {},
   "source": [
    "#station_info = pd.read_csv(\"../data/stations_rhine_elbe.csv\")\n",
    "# Random sampling all_stations (70/30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e87b4dbf-b3cc-43ff-a63b-7bdef512d972",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m setup \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall_stations\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m output_base_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../R/data/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msetup\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 6\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mmakedirs(output_base_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m subsample \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m6\u001b[39m):\n\u001b[0;32m      9\u001b[0m     output_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_base_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubsample_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubsample\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# Iterate over sub-samples\n",
    "# Path configurations\n",
    "setup = \"all_stations\"\n",
    "\n",
    "output_base_dir = f'../R/data/{setup}/'\n",
    "os.makedirs(output_base_dir, exist_ok=True)\n",
    "\n",
    "for subsample in range(1, 6):\n",
    "    output_dir = os.path.join(output_base_dir, f'subsample_{subsample}')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    print(f'Sampling Subsample {subsample}...')\n",
    "    \n",
    "    ## Subset train stations randomly:\n",
    "    train_stations = random.sample(list(station_info['grdc_no']), 35)\n",
    "    train_stations = station_info[station_info['grdc_no'].isin(train_stations)]\n",
    "    \n",
    "    # Subset test stations\n",
    "    test_stations = station_info[~station_info['grdc_no'].isin(train_stations['grdc_no'])]\n",
    "    \n",
    "    \n",
    "    # Create train table\n",
    "    train_table = subsample_table(train_stations, file_paths, station_info)\n",
    "    train_table['datetime'] = pd.to_datetime(train_table['datetime']).dt.date\n",
    "    \n",
    "    # Create train test\n",
    "    test_table = subsample_table(test_stations, file_paths,station_info)\n",
    "    test_table['datetime'] = pd.to_datetime(test_table['datetime']).dt.date\n",
    "    \n",
    "\n",
    "    nrow_train = train_table.shape[0]\n",
    "    nrow_test = test_table.shape[0]\n",
    "    \n",
    "    ratio_subsamples = nrow_train / (nrow_train + nrow_test)\n",
    "    \n",
    "    print(ratio_subsamples)\n",
    "    \n",
    "    # Sample file paths for test stations\n",
    "    test_file_paths = random.sample(file_paths, k=len(test_stations))\n",
    "    \n",
    "    # Filter file paths for train stations\n",
    "    train_file_paths = [file_path for file_path in file_paths if file_path not in test_file_paths]\n",
    "    \n",
    "    # Write tables: train_stations, test_stations, train_table\n",
    "    train_stations.to_csv(os.path.join(output_dir, 'train_stations.csv'), index=False)\n",
    "    test_stations.to_csv(os.path.join(output_dir, 'test_stations.csv'), index=False)\n",
    "    train_table.to_csv(os.path.join(output_dir, 'train_table_allpredictors.csv'), index=False)\n",
    "    test_table.to_csv(os.path.join(output_dir, 'test_table_allpredictors.csv'), index=False)\n",
    "\n",
    "    # Save test file paths\n",
    "    with open(os.path.join(output_dir, 'test_file_paths.txt'), 'w') as f:\n",
    "        for file_path in test_file_paths:\n",
    "            f.write(file_path + '\\n')\n",
    "    \n",
    "     \n",
    "    # Save train file paths\n",
    "    with open(os.path.join(output_dir, 'train_file_paths.txt'), 'w') as f:\n",
    "        for file_path in train_file_paths:\n",
    "            f.write(file_path + '\\n')\n",
    "\n",
    "    print(f'Finished Subsample {subsample}...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3c37ac64-5a25-4399-8db0-8fecd5130de5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{6351: 21, 6401: 10, 6211: 5}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subregion_train_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "96d21764-e139-4699-aed9-257fefbb3ea0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56a93293-0217-4cdf-b28c-9eba53b23941",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "06bdfdf3-a7dc-430b-a37b-b0f43df90b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "5\n",
      "21\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2772a175-8281-47d2-85bb-c7fa0d96d227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup Maas_lag6...\n"
     ]
    }
   ],
   "source": [
    "################\n",
    "setup = \"Maas_lag6\"  # 2, 3 and 4 => test stations for rhine, elbe, mass respectively\n",
    "\n",
    "output_dir = f'../R/data/{setup}/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "print(f'Setup {setup}...')\n",
    "\n",
    "# Select test stations\n",
    "if setup == \"Elbe_lag6\":\n",
    "    test_stations = elbe_stations\n",
    "elif setup == \"Maas_lag6\":\n",
    "    test_stations = Maas_stations\n",
    "\n",
    "\n",
    "################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ff2245-c83d-4e32-978e-089a0d6574a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8638715967899198\n"
     ]
    }
   ],
   "source": [
    "#station_info[~station_info['grdc_no'].isin(test_stations['grdc_no'])]\n",
    "\n",
    "# Create train stations\n",
    "train_stations = rhine_stations\n",
    "\n",
    "# Create train table\n",
    "train_table = subsample_table(train_stations, file_paths, station_info)\n",
    "train_table['datetime'] = pd.to_datetime(train_table['datetime']).dt.date\n",
    "\n",
    " # Create train test\n",
    "test_table = subsample_table(test_stations, file_paths,station_info)\n",
    "test_table['datetime'] = pd.to_datetime(test_table['datetime']).dt.date\n",
    "\n",
    "\n",
    "nrow_train = train_table.shape[0]\n",
    "nrow_test = test_table.shape[0]\n",
    "\n",
    "ratio_subsamples = nrow_train / (nrow_train + nrow_test)\n",
    "\n",
    "print(ratio_subsamples)\n",
    "\n",
    "# Sample file paths for test stations\n",
    "test_file_paths = random.sample(file_paths, k=len(test_stations))\n",
    "\n",
    "# Filter file paths for train stations\n",
    "train_file_paths = [file_path for file_path in file_paths if file_path not in test_file_paths]\n",
    "\n",
    "# Write tables: train_stations, test_stations, train_table\n",
    "train_stations.to_csv(os.path.join(output_dir, 'train_stations.csv'), index=False)\n",
    "test_stations.to_csv(os.path.join(output_dir, 'test_stations.csv'), index=False)\n",
    "train_table.to_csv(os.path.join(output_dir, 'train_table_allpredictors.csv'), index=False)\n",
    "test_table.to_csv(os.path.join(output_dir, 'test_table_allpredictors.csv'), index=False)\n",
    "\n",
    "# Save test file paths\n",
    "with open(os.path.join(output_dir, 'test_file_paths.txt'), 'w') as f:\n",
    "    for file_path in test_file_paths:\n",
    "        f.write(file_path + '\\n')\n",
    "\n",
    "\n",
    "# Save train file paths\n",
    "with open(os.path.join(output_dir, 'train_file_paths.txt'), 'w') as f:\n",
    "    for file_path in train_file_paths:\n",
    "        f.write(file_path + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
