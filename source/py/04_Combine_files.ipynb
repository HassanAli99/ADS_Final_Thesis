{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff713f79-e4c4-42ea-9430-a7966c87565f",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "## 4. Combine variables per station (including observed discharge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9299b617-22a8-43c1-bff7-1875d4e3e3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "from os.path import join as pjoin # Joining file directories\n",
    "import glob\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29288697-3d33-4d23-9af0-0ca404135799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set your working directory and go there\n",
    "work_dir = \"data\"\n",
    "os.chdir(work_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37e1e504-c09f-4d0e-96b1-d0183dd9f208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the station latitude and longitude from a CSV file\n",
    "loc = pd.read_csv('stationLatLon.csv')\n",
    "\n",
    "sub_regions = [6351, 6361, 6401, 6211] \n",
    "loc = loc[(loc.wmo_reg == 6) & (loc.lat.between(45, 54.5)) & (loc.lon.between(4, 15.5)) & (loc.sub_reg.isin(sub_regions)) & (loc.river != \"HAVEL\")]\n",
    "loc.to_csv(\"stations_rhine_elbe.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d9a1b2a-985b-40a0-9111-9c66a25584b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_desired = loc[\"grdc_no\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5774c28b-9c17-4511-90fa-76a0afa8bc4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stations_desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3bcf807-4dca-4f47-a63a-9ca1a2dbc957",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dis(column, area):\n",
    "    time = 24 * 3600\n",
    "    area_m = area*1000000\n",
    "    new_column = column * (time / area_m)\n",
    "    return new_column\n",
    "\n",
    "\n",
    "def normalize_columns_with_dis(df, area):\n",
    "    for column in df.columns:\n",
    "        if 'dis' in column:\n",
    "            df[column] = normalize_dis(df[column], area)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa53663b-9cba-49e4-8dae-747c7578da3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "def combine_models(models):\n",
    "    loc = pd.read_csv('stationLatLon.csv')\n",
    "    loc = loc[(loc.wmo_reg == 6) & (loc.lat.between(45, 54.5)) & (loc.lon.between(4, 15.5))]\n",
    "    stations_desired = loc[\"grdc_no\"].to_list()\n",
    "    combined_dfs = {}\n",
    "\n",
    "    for model in models:\n",
    "        input_folder = f'{model}/upstream_station_all/'\n",
    "        file_pattern = f\"{input_folder}*.csv\"\n",
    "        file_list = glob.glob(file_pattern)\n",
    "\n",
    "        for path_csv_file in file_list:\n",
    "            station = int(os.path.basename(path_csv_file).split(\"_\")[0])\n",
    "\n",
    "            # Check if the station is in the desired list\n",
    "            if station in stations_desired:\n",
    "                # Read the CSV file\n",
    "                df = pd.read_csv(path_csv_file)\n",
    "\n",
    "            \n",
    "                area_km = loc.loc[loc[\"grdc_no\"] == station].reset_index(drop=True)[\"area\"][0]\n",
    "                df = normalize_columns_with_dis(df, area_km)  # Apply normalization here\n",
    "\n",
    "                # Check if the station already exists in the combined_dfs dictionary\n",
    "                if station in combined_dfs:\n",
    "                    # Concatenate the current data with the existing data for the station\n",
    "                    combined_dfs[station] = pd.concat([combined_dfs[station], df], axis=1)\n",
    "                else:\n",
    "                    # Add the data to the combined_dfs dictionary\n",
    "                    combined_dfs[station] = df\n",
    "\n",
    "    # Save the combined data for each station to separate CSV files\n",
    "    output_folder = 'raw_data/predictors/combined'\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    for station, data in combined_dfs.items():\n",
    "        output_file = os.path.join(output_folder, f\"allpredictors_{station}.csv\")\n",
    "        data = data.loc[:, ~data.columns.duplicated()]\n",
    "        data['datetime'] = pd.to_datetime(data['datetime'], format='%Y_%m')\n",
    "        data.to_csv(output_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "969a624e-4d18-49d6-a12f-98961526ae84",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"meteo\", \"wg3\", \"lis\", \"pcr\", \"grdc\"]\n",
    "combine_models(models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
