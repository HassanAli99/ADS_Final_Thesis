{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "891c0d20-e5b0-4e67-bc99-490638085b11",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "## 6. Sampling data for modelling\n",
    "\n",
    "- In this file we create the valiation dataset for each setup described in the thesis. The table below shows the folder names and the setups that were created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6141f1-78c9-4304-b135-3a268daac5b2",
   "metadata": {},
   "source": [
    "| id | Folder name        | Setup             | Description                                                                                                                      |\n",
    "|----|--------------------|-------------------|----------------------------------------------------------------------------------------------------------------------------------|\n",
    "| 1  | allpredictors      | -                 | Folder containing allpredictors tables for all stations within the study area. Variables include GHMs outputs, meteorological variables and observed discharge.                                                                                                               |\n",
    "| 2  | allpredictors_pcr  | -                 | Folder containing PCR_allpredictors table from Magni et al. (2023) dataset. All stations with the study area.                                                                                       |\n",
    "| 3  | all_predCachAtt    | -                 | Folder containing data from allpredictors and catchment attributes from Magni et al. (2023) dataset. All stations with the study area.                                                               |\n",
    "| 4  | all_stations       | all_stations      | Cross-validation dataset of all_stations setup containing 5 subsamples of all the dataset within the study area.                                                                          |\n",
    "| 5  | elbe               | rhine_elbe        | Folder containing training/validation data for rhine_elbe setup.                                                                  |\n",
    "| 6  | elbe_catch         | rhine_elbe_catch  | Folder containing training/validation data for rhine_elbe setup with catchment attributes.                                       |\n",
    "| 7  | maas               | rhine_maas        | Folder containing training/validation data for rhine_maas setup.                                                                  |\n",
    "| 8  | maas_catch         | rhine_maas_catch  | Folder containing training/validation data for rhine_maas setup with catchment attributes.                                       |\n",
    "| 9  | rhine_catch        | rhine_only_catch  | Cross-validation dataset of rhine_only setup with catchment attributes containing 5 subsamples of all stations within rhine basin. |\n",
    "| 10 | rhine_only         | allpredictors_catch | Cross-validation dataset of rhine_only setup containing 5 subsamples of all stations within rhine basin.                            |\n",
    "| 11 | rhine_pcr          | PCR_allpredictors | Cross-validation dataset of rhine_only setup with catchment attributes containing 5 subsamples of all stations within rhine basin. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac95e704-7206-4c7c-b669-5e75869a46e6",
   "metadata": {},
   "source": [
    "**Download folder: raw_data data that contains results from all the previous pre-processing steps. We sample from the following folders:**\n",
    "\n",
    "    - allpredictors\n",
    "    - allpredictors_pcr\n",
    "    - allpredCatchAtt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9912f46-baa9-47d9-98cb-3de8a142a06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import pandas as pd\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cec217e-dc36-4b6c-86ee-fa08efa35eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_dict = {\n",
    "    1: \"all_stations\",\n",
    "    2: \"elbe\",\n",
    "    3: \"elbe_catch\",\n",
    "    4: \"maas\",\n",
    "    5: \"maas_catch\",\n",
    "    6: \"rhine_catch\",\n",
    "    7: \"rhine_only\",\n",
    "    8: \"rhine_pcr\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f9db978-21b1-45a0-b66f-cd0b0e9bf0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 123 \n",
    "random.seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e31f78b-57c1-49d6-b4c6-05299dba05f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dis(column, area):\n",
    "    time = 24 * 3600\n",
    "    area_m = area*1000000\n",
    "    new_column = column * (time / area_m)\n",
    "    return new_column\n",
    "\n",
    "\n",
    "def normalize_columns_with_dis(df, area):\n",
    "    for column in df.columns:\n",
    "        if 'dis' in column:\n",
    "            df[column] = normalize_dis(df[column], area)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fadfa242-5cfa-46db-a974-690614d04289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsample_table(stations, filenames, area):\n",
    "    grdc_nos = [str(grdc_no) for grdc_no in stations['grdc_no']]\n",
    "    sub_filenames = [filename for filename in filenames if any(grdc_no in filename for grdc_no in grdc_nos)]\n",
    "    sub_datas = []\n",
    "    \n",
    "    \n",
    "    for filename in sub_filenames:\n",
    "        sub_data = pd.read_csv(filename)\n",
    "        \n",
    "        # Convert 'datetime' column to datetime type\n",
    "        sub_data['datetime'] = pd.to_datetime(sub_data['datetime'])\n",
    "        \n",
    "        # Subset the data based on the datetime range\n",
    "        start_date = pd.to_datetime('1979-01-01')\n",
    "        end_date = pd.to_datetime('2012-12-31')\n",
    "        sub_data = sub_data[(sub_data['datetime'] >= start_date) & (sub_data['datetime'] <= end_date)]\n",
    "        \n",
    "        \n",
    "        sub_datas.append(sub_data)\n",
    "    \n",
    "    sub_table = pd.concat(sub_datas, ignore_index=True)\n",
    "    \n",
    "    return sub_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "583c0217-f552-4e3f-9d4b-8db18bdda75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_path_preds(setup_name):\n",
    "    if setup_name:\n",
    "        if 'catch' in setup_name:\n",
    "            file_path_preds = '../raw_data/all_predCachAtt/'\n",
    "        elif 'pcr' in setup_name:\n",
    "            file_path_preds = '../raw_data/allpredictors_pcr/'\n",
    "        else:\n",
    "            file_path_preds = '../raw_data/allpredictors/' \n",
    "            \n",
    "        return file_path_preds\n",
    "        print(f\"Setup ID: {setup_id}, Setup Name: {setup_name}, File Path Preds: {file_path_preds}\")\n",
    "    else:\n",
    "        print(f\"Invalid Setup ID: {setup_id}\")\n",
    "\n",
    "\n",
    "setup_dict = {\n",
    "    1: \"all_stations\",\n",
    "    2: \"elbe\",\n",
    "    3: \"elbe_catch\",\n",
    "    4: \"maas\",\n",
    "    5: \"maas_catch\",\n",
    "    6: \"rhine_catch\",\n",
    "    7: \"rhine_only\",\n",
    "    8: \"rhine_pcr\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d530586a-3272-4279-bbdb-f5c1ace27fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_info = pd.read_csv(\"../../../data/stations_rhine_elbe.csv\")\n",
    "rhine_stations = station_info.loc[(station_info[\"sub_reg\"] == 6351) | (station_info[\"sub_reg\"] == 6361)]\n",
    "elbe_stations = station_info.loc[(station_info[\"sub_reg\"] == 6401)]\n",
    "maas_stations = station_info.loc[(station_info[\"sub_reg\"] == 6211)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84c6be43-2069-4335-970b-a6e707124d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "elbe_stations = elbe_stations[~elbe_stations[\"grdc_no\"].isin([6340300, 6340301])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c65fdb4-abfa-411b-b6f1-437d6405d96c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Rhine subsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "624fd794-9e70-4734-a001-0eb3f08b4988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rhine_pcr'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setup_rhine = setup_dict[8] # 6, 7 or 8\n",
    "setup_rhine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1d762add-5acb-4f67-b0f8-7e6e3d8aab92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../data/allpredictors_pcr/\n"
     ]
    }
   ],
   "source": [
    "# File paths\n",
    "file_path_preds = get_file_path_preds(setup_rhine)\n",
    "print(file_path_preds)\n",
    "file_list_preds = os.listdir(file_path_preds)\n",
    "file_paths = [os.path.join(file_path_preds, file) for file in file_list_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2f29bef1-bcf1-47fa-b441-5754cff919b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ids_rhine = rhine_stations[\"grdc_no\"].to_list()\n",
    "list_ids_all = station_info['grdc_no'].to_list()\n",
    "\n",
    "station_info_rhine = station_info[station_info[\"grdc_no\"].isin([x for x in list_ids_rhine])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9fa25ecc-e8ad-4d00-af73-8cce25cefc84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling Subsample 1...\n",
      "0.7\n",
      "Finished Subsample 1...\n",
      "Sampling Subsample 2...\n",
      "0.7\n",
      "Finished Subsample 2...\n",
      "Sampling Subsample 3...\n",
      "0.7\n",
      "Finished Subsample 3...\n",
      "Sampling Subsample 4...\n",
      "0.7\n",
      "Finished Subsample 4...\n",
      "Sampling Subsample 5...\n",
      "0.7\n",
      "Finished Subsample 5...\n"
     ]
    }
   ],
   "source": [
    "output_base_dir = f'../R/data/{setup_rhine}/'\n",
    "os.makedirs(output_base_dir, exist_ok=True)\n",
    "\n",
    "sample_number = 21\n",
    "\n",
    "for subsample in range(1, 6):\n",
    "    output_dir = os.path.join(output_base_dir, f'subsample_{subsample}')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    print(f'Sampling Subsample {subsample}...')\n",
    "    \n",
    "    ## Subset train stations randomly:\n",
    "    train_station_ids = random.sample(list_ids_rhine, sample_number)\n",
    "    train_stations = station_info_rhine[station_info_rhine['grdc_no'].isin([x for x in train_station_ids])]\n",
    "    \n",
    "    # Subset test stations\n",
    "    test_stations =  station_info_rhine[~station_info_rhine[\"grdc_no\"].isin([x for x in train_station_ids])]\n",
    "    \n",
    "    \n",
    "    # Create train table\n",
    "    train_table = subsample_table(train_stations, file_paths, station_info)\n",
    "    train_table['datetime'] = pd.to_datetime(train_table['datetime']).dt.date\n",
    "    \n",
    "    # Create train test\n",
    "    test_table = subsample_table(test_stations, file_paths,station_info)\n",
    "    test_table['datetime'] = pd.to_datetime(test_table['datetime']).dt.date\n",
    "    \n",
    "\n",
    "    nrow_train = train_table.shape[0]\n",
    "    nrow_test = test_table.shape[0]\n",
    "    \n",
    "    ratio_subsamples = nrow_train / (nrow_train + nrow_test)\n",
    "    \n",
    "    print(ratio_subsamples)\n",
    "    \n",
    "    \n",
    "    # Write tables: train_stations, test_stations, train_table\n",
    "    train_stations.to_csv(os.path.join(output_dir, 'train_stations.csv'), index=False)\n",
    "    test_stations.to_csv(os.path.join(output_dir, 'test_stations.csv'), index=False)\n",
    "    train_table.to_csv(os.path.join(output_dir, 'train_table_allpredictors.csv'), index=False)\n",
    "    test_table.to_csv(os.path.join(output_dir, 'test_table_allpredictors.csv'), index=False)\n",
    "\n",
    "\n",
    "\n",
    "    print(f'Finished Subsample {subsample}...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c85a938-e92b-4be7-8182-6bedb28a1293",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "setup_rhine = setup_dict[6] # 6, 7 or 8\n",
    "setup_rhine#station_info = pd.read_csv(\"../data/stations_rhine_elbe.csv\")\n",
    "# Random sampling all_stations (70/30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b1f30df7-564e-4dd5-b2ee-10ead6a01204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_stations\n",
      "../../../data/allpredictors/\n"
     ]
    }
   ],
   "source": [
    "setup_allstations = setup_dict[1] # 1\n",
    "print(setup_allstations)\n",
    "# File paths\n",
    "file_path_preds = get_file_path_preds(setup_allstations)\n",
    "print(file_path_preds)\n",
    "file_list_preds = os.listdir(file_path_preds)\n",
    "file_paths = [os.path.join(file_path_preds, file) for file in file_list_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e87b4dbf-b3cc-43ff-a63b-7bdef512d972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling Subsample 1...\n",
      "0.6756986054403733\n",
      "Finished Subsample 1...\n",
      "Sampling Subsample 2...\n",
      "0.6950527599554589\n",
      "Finished Subsample 2...\n",
      "Sampling Subsample 3...\n",
      "0.6859324460469802\n",
      "Finished Subsample 3...\n",
      "Sampling Subsample 4...\n",
      "0.6979161143220743\n",
      "Finished Subsample 4...\n",
      "Sampling Subsample 5...\n",
      "0.6879473991197836\n",
      "Finished Subsample 5...\n"
     ]
    }
   ],
   "source": [
    "# Iterate over sub-samples\n",
    "# Path configurations\n",
    "setup = \"all_stations\"\n",
    "\n",
    "output_base_dir = f'../R/data/{setup_allstations}/'\n",
    "os.makedirs(output_base_dir, exist_ok=True)\n",
    "\n",
    "for subsample in range(1, 6):\n",
    "    output_dir = os.path.join(output_base_dir, f'subsample_{subsample}')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    print(f'Sampling Subsample {subsample}...')\n",
    "    \n",
    "    ## Subset train stations randomly:\n",
    "    train_stations = random.sample(list(station_info['grdc_no']), 35)\n",
    "    train_stations = station_info[station_info['grdc_no'].isin(train_stations)]\n",
    "    \n",
    "    # Subset test stations\n",
    "    test_stations = station_info[~station_info['grdc_no'].isin(train_stations['grdc_no'])]\n",
    "    \n",
    "    \n",
    "    # Create train table\n",
    "    train_table = subsample_table(train_stations, file_paths, station_info)\n",
    "    train_table['datetime'] = pd.to_datetime(train_table['datetime']).dt.date\n",
    "    \n",
    "    # Create train test\n",
    "    test_table = subsample_table(test_stations, file_paths,station_info)\n",
    "    test_table['datetime'] = pd.to_datetime(test_table['datetime']).dt.date\n",
    "    \n",
    "\n",
    "    nrow_train = train_table.shape[0]\n",
    "    nrow_test = test_table.shape[0]\n",
    "    \n",
    "    ratio_subsamples = nrow_train / (nrow_train + nrow_test)\n",
    "    \n",
    "    print(ratio_subsamples)\n",
    "    \n",
    "    # Sample file paths for test stations\n",
    "    test_file_paths = random.sample(file_paths, k=len(test_stations))\n",
    "    \n",
    "    # Filter file paths for train stations\n",
    "    train_file_paths = [file_path for file_path in file_paths if file_path not in test_file_paths]\n",
    "    \n",
    "    # Write tables: train_stations, test_stations, train_table\n",
    "    train_stations.to_csv(os.path.join(output_dir, 'train_stations.csv'), index=False)\n",
    "    test_stations.to_csv(os.path.join(output_dir, 'test_stations.csv'), index=False)\n",
    "    train_table.to_csv(os.path.join(output_dir, 'train_table_allpredictors.csv'), index=False)\n",
    "    test_table.to_csv(os.path.join(output_dir, 'test_table_allpredictors.csv'), index=False)\n",
    "\n",
    "    # Save test file paths\n",
    "    with open(os.path.join(output_dir, 'test_file_paths.txt'), 'w') as f:\n",
    "        for file_path in test_file_paths:\n",
    "            f.write(file_path + '\\n')\n",
    "    \n",
    "     \n",
    "    # Save train file paths\n",
    "    with open(os.path.join(output_dir, 'train_file_paths.txt'), 'w') as f:\n",
    "        for file_path in train_file_paths:\n",
    "            f.write(file_path + '\\n')\n",
    "\n",
    "    print(f'Finished Subsample {subsample}...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2a178d-b0ee-44f7-bfab-d21a99c7adc6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Rhine_elbe or Rhine_maas Setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "60ce942a-d8f4-42e0-be86-2e10f1e78108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maas_catch\n",
      "../../../data/all_predCachAtt/\n"
     ]
    }
   ],
   "source": [
    "# Rhine_elbe or Rhine_maas Setups\n",
    "\n",
    "setup_elbe_maas = setup_dict[5] # elbe - 2, 3 or maas - 4, 5\n",
    "\n",
    "print(setup_elbe_maas)\n",
    "# File paths\n",
    "file_path_preds = get_file_path_preds(setup_elbe_maas)\n",
    "print(file_path_preds)\n",
    "file_list_preds = os.listdir(file_path_preds)\n",
    "file_paths = [os.path.join(file_path_preds, file) for file in file_list_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2772a175-8281-47d2-85bb-c7fa0d96d227",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f'../R/data/{setup_elbe_maas}/'\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d1ff2245-c83d-4e32-978e-089a0d6574a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6653017118807939\n"
     ]
    }
   ],
   "source": [
    "#station_info[~station_info['grdc_no'].isin(test_stations['grdc_no'])]\n",
    "\n",
    "# Create train stations\n",
    "train_stations = rhine_stations\n",
    "\n",
    "# Create train table\n",
    "train_table = subsample_table(train_stations, file_paths, station_info)\n",
    "train_table['datetime'] = pd.to_datetime(train_table['datetime']).dt.date\n",
    "\n",
    " # Create train test\n",
    "test_table = subsample_table(test_stations, file_paths,station_info)\n",
    "test_table['datetime'] = pd.to_datetime(test_table['datetime']).dt.date\n",
    "\n",
    "\n",
    "nrow_train = train_table.shape[0]\n",
    "nrow_test = test_table.shape[0]\n",
    "\n",
    "ratio_subsamples = nrow_train / (nrow_train + nrow_test)\n",
    "\n",
    "print(ratio_subsamples)\n",
    "\n",
    "# Sample file paths for test stations\n",
    "test_file_paths = random.sample(file_paths, k=len(test_stations))\n",
    "\n",
    "# Filter file paths for train stations\n",
    "train_file_paths = [file_path for file_path in file_paths if file_path not in test_file_paths]\n",
    "\n",
    "# Write tables: train_stations, test_stations, train_table\n",
    "train_stations.to_csv(os.path.join(output_dir, 'train_stations.csv'), index=False)\n",
    "test_stations.to_csv(os.path.join(output_dir, 'test_stations.csv'), index=False)\n",
    "train_table.to_csv(os.path.join(output_dir, 'train_table_allpredictors.csv'), index=False)\n",
    "test_table.to_csv(os.path.join(output_dir, 'test_table_allpredictors.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ad8c47-1560-4754-a903-41cd97c50197",
   "metadata": {},
   "source": [
    "# All the pre-processed files can be found here: https://zenodo.org/record/8092323"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
